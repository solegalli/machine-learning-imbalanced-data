{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, F-measure, Support ==> multiclass\n",
    "\n",
    "- **Precision** = tp / (tp + fp)\n",
    "\n",
    "- **Recall** = tp / (tp + fn)\n",
    "\n",
    "- **F1** = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "- **Support** = Number of cases on each class\n",
    "\n",
    "\n",
    "By default, sklearn determines the class as the observation with the highest probability value. In this case, it does not depend on a specific threshold value.\n",
    "\n",
    "In this notebook, we will obtain the values of the metrics:\n",
    "\n",
    "- per class\n",
    "- macro averaged\n",
    "- micro averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data (multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = load_wine()\n",
    "\n",
    "data = pd.concat([\n",
    "    pd.DataFrame(data.data, columns=data.feature_names),\n",
    "    pd.DataFrame(data.target, columns=['target']),\n",
    "    ], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    0.398876\n",
       "0    0.331461\n",
       "2    0.269663\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target distribution:\n",
    "# multiclass and (fairly) balanced\n",
    "\n",
    "data.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124, 13), (54, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),  # drop the target\n",
    "    data['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML models\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Produce probability vectors for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59291486, 0.35444264, 0.0526425 ],\n",
       "       [0.12139867, 0.33577091, 0.54283043],\n",
       "       [0.30482504, 0.55905479, 0.13612017],\n",
       "       [0.52711941, 0.38876082, 0.08411977],\n",
       "       [0.27876443, 0.50875176, 0.21248381],\n",
       "       [0.34573413, 0.49743863, 0.15682724],\n",
       "       [0.51144556, 0.3911751 , 0.09737934],\n",
       "       [0.034061  , 0.34869659, 0.6172424 ],\n",
       "       [0.22335574, 0.59578725, 0.18085702],\n",
       "       [0.22335574, 0.59578725, 0.18085702]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the model\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=39, max_depth=1, n_jobs=4)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# produce the predictions (as probabilities)\n",
    "y_train_rf = rf.predict_proba(X_train)\n",
    "y_test_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# note that the predictions are an array of 3 columns\n",
    "\n",
    "# first column: the probability of an observation of being of class 0\n",
    "# second column: the probability of an observation of being of class 1\n",
    "# third column: the probability of an observation of being of class 2\n",
    "\n",
    "y_test_rf[0:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 0, 1, 1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The final prediction is that of the biggest probabiity\n",
    "\n",
    "rf.predict(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The Logistic regression also support multiclass targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sole\\Documents\\Repositories\\envs\\fsml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sole\\Documents\\Repositories\\envs\\fsml\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.91180082e-01, 1.25239040e-03, 7.56752742e-03],\n",
       "       [2.39665961e-06, 9.24182544e-08, 9.99997511e-01],\n",
       "       [6.72505554e-03, 9.91780725e-01, 1.49421954e-03],\n",
       "       [9.85740546e-01, 2.01772702e-03, 1.22417272e-02],\n",
       "       [4.71428271e-03, 9.93594199e-01, 1.69151866e-03],\n",
       "       [7.83097269e-04, 9.92144233e-01, 7.07266924e-03],\n",
       "       [9.99369417e-01, 1.30593135e-04, 4.99990091e-04],\n",
       "       [7.52750289e-04, 5.63891582e-05, 9.99190861e-01],\n",
       "       [1.56499288e-02, 9.81839514e-01, 2.51055713e-03],\n",
       "       [4.08763622e-03, 9.94374297e-01, 1.53806686e-03]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the model\n",
    "logit = LogisticRegression(\n",
    "    random_state=0, multi_class='multinomial', max_iter=100,\n",
    ")\n",
    "\n",
    "# train\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# obtain the probabilities\n",
    "y_train_logit = logit.predict_proba(X_train)\n",
    "y_test_logit = logit.predict_proba(X_test)\n",
    "\n",
    "# note that the predictions are an array of 3 columns\n",
    "\n",
    "# first column: the probability of an observation of being of class 0\n",
    "# second column: the probability of an observation of being of class 1\n",
    "# third column: the probability of an observation of being of class 2\n",
    "\n",
    "y_test_logit[0:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 0, 1, 1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The final prediction is that of the biggest probabiity\n",
    "\n",
    "logit.predict(X_test)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rest of the notebook, we work with the class predictions\n",
    "\n",
    "y_rf_pred = rf.predict(X_test)\n",
    "\n",
    "y_logit_pred = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  [1.         0.86956522 0.85714286]\n",
      "Recall:  [0.89473684 0.90909091 0.92307692]\n",
      "f score:  [0.94444444 0.88888889 0.88888889]\n",
      "Support:  [19 22 13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    y_rf_pred,\n",
    "    labels=[0,1,2], # the labels for which we want the metrics determined\n",
    "    average=None, # when None, returns a metric per label\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro\n",
    "\n",
    "Take the average of the individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9089026915113871\n",
      "Recall:  0.9089682247576985\n",
      "f score:  0.9074074074074074\n",
      "Support:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    y_rf_pred,\n",
    "    labels=[0,1,2], # the labels for which we want the metrics determined\n",
    "    average='macro', # take the average of the metrics\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9089026933333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For precision this is the same as:\n",
    "\n",
    "(1. + 0.86956522 + 0.85714286) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighed average\n",
    "\n",
    "Takes the average of each metric weighted by the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9124683689901082\n",
      "Recall:  0.9074074074074074\n",
      "f score:  0.9084362139917695\n",
      "Support:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    y_rf_pred,\n",
    "    labels=[0,1,2], # the labels for which we want the metrics determined\n",
    "    average='weighted', # take the average of the metrics\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124683707407407"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For precision this is the same as:\n",
    "\n",
    "(19 * 1. + 22 * 0.86956522 + 13 * 0.85714286) / (19+22+13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro\n",
    "\n",
    "Collective average of TP, FP anf FN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9074074074074074\n",
      "Recall:  0.9074074074074074\n",
      "f score:  0.9074074074074074\n",
      "Support:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we determine the metrics for each one of the classes\n",
    "# just like we did in the intro video\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test, y_rf_pred, labels=[0,1,2], average='micro',\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "### Recall, Precision and f score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9089026915113871\n",
      "Recall:  0.9089682247576985\n",
      "f score:  0.9074074074074074\n",
      "Support:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forests\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    y_rf_pred,\n",
    "    labels=[0,1,2], # the labels for which we want the metrics determined\n",
    "    average='macro', # take the average of the metrics\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9472049689440993\n",
      "Recall:  0.9497607655502392\n",
      "f score:  0.9469135802469135\n",
      "Support:  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(\n",
    "    y_test,\n",
    "    y_logit_pred,\n",
    "    labels=[0,1,2], # the labels for which we want the metrics determined\n",
    "    average='macro', # take the average of the metrics\n",
    ")\n",
    "\n",
    "print('Precision: ', p)\n",
    "print('Recall: ', r)\n",
    "print('f score: ', f)\n",
    "print('Support: ', s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression seems to be working a bit better on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest test: 0.9074074074074074\n",
      "Accuracy Logistic Regression test: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Random Forest test:', accuracy_score(y_test, rf.predict(X_test)))\n",
    "print('Accuracy Logistic Regression test:', accuracy_score(y_test, logit.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy, Random Forest test: 0.9089682247576985\n",
      "Balanced accuracy, Regression test: 0.9497607655502392\n"
     ]
    }
   ],
   "source": [
    "print('Balanced accuracy, Random Forest test:', balanced_accuracy_score(y_test, rf.predict(X_test)))\n",
    "print('Balanced accuracy, Regression test:',  balanced_accuracy_score(y_test, logit.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsml",
   "language": "python",
   "name": "fsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
