{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification cost as part of training\n",
    "\n",
    "There are 2 ways in which we can introduce cost into the learning function of the algorithm with Scikit-learn:\n",
    "\n",
    "- Defining the **class_weight** parameter for those estimators that allow it, when we set the estimator\n",
    "- Passing a **sample_weight** vector with the weights for every single observation, when we fit the estimator.\n",
    "\n",
    "\n",
    "With both the **class_weight** parameter or the **sample_weight** vector, we indicate that the loss function should be modified to accommodate the class imbalance and the cost attributed to each misclassification.\n",
    "\n",
    "## parameters\n",
    "\n",
    "**class_weight**: can take 'balanced' as argument, in which case it will use the balance ratio as weight. Alternatively, it can take a dictionary with {class: penalty}, pairs. In this case, it penalizes mistakes in samples of class[i] with class_weight[i].\n",
    "\n",
    "So if class_weight = {0:1, and 1:10}, misclassification of observations of class 1 are penalized 10 times more than misclassification of observations of class 0.\n",
    "\n",
    "**sample_weight** is a vector of the same length as y, containing the weight or penalty for each individual observation. In principle, it is more flexible, because it allows us to set weights to the observations and not to the class as a whole. So in this case, for example we could set up higher penalties for fraudulent applications that are more costly (money-wise)than to those fraudulent applications that are of little money.\n",
    "\n",
    "## Important\n",
    "\n",
    "If you use both class_weight and sample_weight, the final penalty will be **the combination of the 2**, so be very careful\n",
    "\n",
    "## Demo\n",
    "\n",
    "In this demo, I will introduce cost-sensitive learning to Logistic Regression. But keep in mind that you can do the same with almost every other classifier in Scikit-learn using **sample_weight** or, using **Class_weight** in those estimators that have that attribute.\n",
    "\n",
    "## Classifiers that support class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sole\\Documents\\Repositories\\envs\\imbalanced\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "LinearSVC\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "NuSVC\n",
      "PassiveAggressiveClassifier\n",
      "Perceptron\n",
      "RandomForestClassifier\n",
      "RidgeClassifier\n",
      "RidgeClassifierCV\n",
      "SGDClassifier\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "# NOTE, THIS CELL WILL NOT WORK WITH SKLEARN VERSION > 0.24\n",
    "# JUST COMMENT IT OUT, OR DELETE THIS CELL TO CARRY ON WITH THE NOTEBOOK.\n",
    "\n",
    "# Let's find out which classifiers from sklearn support class_weight\n",
    "# as part of the __init__ method, that is, when we set the m up\n",
    "\n",
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    try:\n",
    "        if hasattr(class_(), 'class_weight'):\n",
    "            print(name)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all classifiers support class_weight. For those which don't, like GradientBoostingClassifier, we can still use sample_weight when we fit the estimator.\n",
    "\n",
    "## Demo\n",
    "\n",
    "In this demo, we are going to introduce the misclassification cost in Logistic Regression, using class_weight and then sample_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39638</th>\n",
       "      <td>16.59</td>\n",
       "      <td>38.89</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>2631.7</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>1.49</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2439.9</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>734.3</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66943</th>\n",
       "      <td>78.14</td>\n",
       "      <td>24.35</td>\n",
       "      <td>0.73</td>\n",
       "      <td>46.5</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>2012.2</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1593.8</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>2.08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>1114.6</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63322</th>\n",
       "      <td>19.08</td>\n",
       "      <td>34.48</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1268.7</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>747.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-88.0</td>\n",
       "      <td>573.4</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51625</th>\n",
       "      <td>65.28</td>\n",
       "      <td>22.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>1763.2</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-63.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1103.5</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>511.1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48887</th>\n",
       "      <td>73.60</td>\n",
       "      <td>26.90</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>1341.7</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1594.4</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>608.6</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1     2     3     4       5     6     7     8     9  ...  \\\n",
       "39638  16.59  38.89  1.20  -3.5  33.5  2631.7 -0.79  1.49  12.0 -73.0  ...   \n",
       "66943  78.14  24.35  0.73  46.5 -54.0  2012.2 -1.07 -0.11   8.0 -94.0  ...   \n",
       "63322  19.08  34.48 -0.63 -29.0   4.5  1268.7 -0.32 -0.82 -32.0 -52.5  ...   \n",
       "51625  65.28  22.37  0.40 -32.0 -18.0  1763.2 -0.60 -1.14 -24.0 -63.5  ...   \n",
       "48887  73.60  26.90 -1.02   7.0  38.5  1341.7  0.51 -0.28  -3.5 -66.5  ...   \n",
       "\n",
       "           65    66    67    68    69      70    71    72    73  target  \n",
       "39638  2439.9  0.39 -0.76  -8.0 -78.0   734.3  0.59  0.22 -0.74      -1  \n",
       "66943  1593.8 -1.14  2.08  11.0 -92.0  1114.6 -1.04  0.27  0.36      -1  \n",
       "63322   747.0  0.16  2.33   3.0 -88.0   573.4 -0.14 -0.04  0.04      -1  \n",
       "51625  1103.5  0.53 -0.12   1.0 -64.0   511.1  0.49  0.27  0.12      -1  \n",
       "48887  1594.4 -0.44 -0.35   8.0 -62.0   608.6  0.92  0.20  0.26      -1  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "# only a few observations to speed the computaton\n",
    "\n",
    "data = pd.read_csv('../kdd2004.csv').sample(10000)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    0.9903\n",
       " 1    0.0097\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalanced target\n",
    "\n",
    "data.target.value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 74), (3000, 74))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),  # drop the target\n",
    "    data['target'],  # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with class_weight\n",
    "\n",
    "# we initialize the cost / weights when we set up the transformer\n",
    "\n",
    "def run_Logit(X_train, X_test, y_train, y_test, class_weight):\n",
    "    \n",
    "    # weights introduced here\n",
    "    logit = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='newton-cg',\n",
    "        random_state=0,\n",
    "        max_iter=10,\n",
    "        n_jobs=4,\n",
    "        class_weight=class_weight # weights / cost\n",
    "    )\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "\n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(X_train)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "\n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(X_test)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9192043838780551\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8979334677419354\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# using imbalanced dataset\n",
    "\n",
    "run_Logit(X_train,\n",
    "          X_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9925445596049605\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9620855734767024\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# cost estimated as imbalance ratio\n",
    "\n",
    "# 'balanced' indicates that we want same amount of \n",
    "# each observation, thus, imbalance ratio\n",
    "\n",
    "run_Logit(X_train,\n",
    "          X_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9617874072272288\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9445704525089607\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# cost estimated as imbalance ratio\n",
    "\n",
    "# alternatively, we can pass a different cost\n",
    "# in a dictionary, if we know it already\n",
    "\n",
    "run_Logit(X_train,\n",
    "          X_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          class_weight={-1:1, 1:10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the cost and see what you get in terms of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression + sample_weight\n",
    "\n",
    "# we pass the weights / cost, when we train the algorithm\n",
    "\n",
    "def run_Logit(X_train, X_test, y_train, y_test, sample_weight):\n",
    "    \n",
    "    logit = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='newton-cg',\n",
    "        random_state=0,\n",
    "        max_iter=10,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    \n",
    "    # costs are passed here\n",
    "    logit.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(X_train)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "\n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(X_test)\n",
    "    print(\n",
    "        'Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9192043838780551\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8979334677419354\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# using imbalanced dataset\n",
    "\n",
    "run_Logit(X_train,\n",
    "          X_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.992609819428047\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9542450716845878\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of algorithm built\n",
    "# cost estimated as imbalance ratio\n",
    "\n",
    "# with numpy.where, we introduce a cost of 99 to\n",
    "# each observation of the minority class, and 1\n",
    "# otherwise.\n",
    "\n",
    "run_Logit(X_train,\n",
    "          X_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          sample_weight=np.where(y_train==1,99,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cost-sensitive learning has improved the performance of the model.\n",
    "\n",
    "**HOMEWORK**\n",
    "\n",
    "Try other machine learning algorithms and other datasets available in imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imbalanced",
   "language": "python",
   "name": "imbalanced"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
